# Dataset V0 â€“ Sentiment Analysis

## ğŸ“Œ Source
- **Nom** : Sentiment140 Dataset  
- **Lien** : [https://www.kaggle.com/datasets/kazanova/sentiment140](https://www.kaggle.com/datasets/kazanova/sentiment140)  
- **Format** : CSV (~1.6 million de tweets)  

## ğŸ“Š Taille utilisÃ©e pour V0
- Pour le **prototype initial (V0)**, nous utilisons **2 000 tweets Ã©chantillonnÃ©s**.  
- Cela permet :
  - dâ€™accÃ©lÃ©rer lâ€™entraÃ®nement du baseline,
  - dâ€™Ã©viter les temps longs dâ€™exÃ©cution,
  - de valider rapidement la faisabilitÃ© du projet.

## ğŸŒ Langue
- Anglais (tweets majoritairement en anglais).
- Avantage : simplicitÃ© pour un premier prototype (outils NLP bien adaptÃ©s).

## âš ï¸ ProblÃ¨mes connus
- DonnÃ©es bruyantes :  
  - doublons,  
  - tweets vides ou trÃ¨s courts,  
  - emojis, hashtags, URLs, mentions (@user),  
  - mÃ©lange de langues dans certains cas.  
- Ces problÃ¨mes seront traitÃ©s progressivement dans les Ã©tapes de prÃ©traitement NLP.

## âœ… Justification du choix
- **Ouvert et libre** : dataset disponible publiquement sur Kaggle.  
- **RÃ©putation** : largement utilisÃ© dans la recherche en sentiment analysis.  
- **Volume suffisant** : 1,6M tweets si besoin pour passer Ã  lâ€™Ã©chelle (versions futures).  
- **Baseline rapide** : possibilitÃ© dâ€™utiliser un petit Ã©chantillon (2k) pour dÃ©marrer rapidement.  

## ğŸš€ Perspectives (Versions futures)
- V1 : augmenter Ã  50k tweets pour un modÃ¨le plus robuste.  
- V2+ : utiliser le dataset complet (1.6M) + collecte en temps rÃ©el (API Twitter/Reddit).  
